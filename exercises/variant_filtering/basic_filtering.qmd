---
title: Basic variant filtering
subtitle: Basic filtering based on information in VCF file
description: >-
  Introduction to basic variant filtering using the CLI.
format:
  nbis-course-html:
    number-sections: false
author:
  - Per Unneberg
execute:
  cache: false
exercise:
  dir: variant-filtering
  venv: e-variant-filtering
  data: monkeyflower/variant-filtering/
  wget_opts: \-\-cut-dirs=5
---

<!-- markdownlint-disable MD041 -->

{{< include /exercises/_knitr.qmd >}}

{{< include /exercises/_rbuild.qmd >}}

{{< include /exercises/_rlibs.qmd >}}

```{r }
#| label: r-init-vars
#| echo: false
#| eval: true
obj <- exercise_tools$variant_filtering$basic_filtering
bioinfo_tools <- TRUE
```

<!-- markdownlint-enable MD041 -->

## About

In this exercise we will look at ways of filtering variant data. We
will begin by applying filters to the variant file containing variant
sites only, followed by an approach that filters on sequencing depth
in a variant file containing both variant and invariant sites. The
latter methodology can then be generalized to generate depth-based
filters from BAM files.

{{< include /exercises/_subset.qmd >}}

::: {.callout-tip collapse=true}

## Learning objectives

- filter variants by quality, read depth, and other metrics
- apply filters to a VCF file

:::

{{< include /exercises/_tools.qmd >}}

{{< include /exercises/_datasetup.qmd >}}

## Background

Regardless of how a raw variant call set has been produced, the calls
will be of varying quality for a number of reasons. For high-coverage
sequencing, the two most common are incompleteness of the reference
sequence and misalignments in repetitive regions
[@li_BetterUnderstandingArtifacts_2014]. Low-coverage sequencing comes
with its own biases and issues, with the most important being the
difficulty to accurately call
genotypes [@maruki_GenotypeCallingPopulationGenomic_2017].

In order to improve the accuracy of downstream inference, a number of
analysis-dependent quality control filters should be applied to the
raw variant call set (for a concise summary, see
@lou_BeginnerGuideLowcoverage_2021). In this exercise, we will begin
by applying filters to the variant file containing variant sites only,
followed by a more general approach based on depth filtering of a
variant file consisting of all sites, variant as well as invariant.

It is worthwhile to spend time thinking about filtering. As we will
see, there are numerous metrics to filter on, and different
applications require different filters. This is not as straightforward
as it first may seem, and even experts struggle to get filtering
settings right.

### Some recommended data filters

There are many ways to filter data. Choosing the right set of filters
is not easy, and choosing appropriate thresholds depends on
application, among other things. Below we list some recommended data
filters and thresholds that have general applicability and recently
have been reviewed [@lou_BeginnerGuideLowcoverage_2021, Table 3]; but
see also [@hemstrom_NextgenerationDataFiltering_2024]:

- **depth**: Given the difficulty of accurately genotyping
  low-coverage sites, it is recommended to set a minimum read depth
  cutoff to remove false positive calls. It is also recommended to set
  a maximum depth cutoff as excessive coverage is often due to
  mappings to repetitive regions. The thresholds will depend on the
  depth profile over all sites, but is usually chosen as a range
  around the mean or median depth (e.g. lower threshold 0.8X mean,
  upper threshold median + 2 standard deviations).
- **minimum number of individuals**: To avoid sites with too much
  missing data across individuals, a common requirement is that a
  minimum number (fraction) of individuals, say 75%, have sequence
  coverage (depth-based filter) or genotype calls.
- **quality** (***p*-value**): Most variant calling software provide a
  Phred-scaled probability score that a genotype is a true genotype.
  Quality values below 20 (i.e., 1%) should not be trusted, but could
  be set much higher (i.e., lower *p*-value) depending on application.
  Note that if a VCF file includes invariant sites, they have quality
  values set to 0, which renders quality based filtering
  inappropriate.
- **MAF**: Filter sites based on a minimum minor allele frequency
  (MAF) threshold. The appropriate choice depends on application. For
  instance, for PCA or admixture analyses, low-frequency SNPs are
  uninformative, and a reasonably large cutoff (say, 0.05-0.10) could
  be set. If an analysis depends on invariant sites, this filter
  should not be applied.

::: {.callout-important}

For applications where invariant sites should be included, such as
genetic diversity calculations, neither quality nor MAF filtering
should be applied.

:::

## Basic filtering of a VCF file[^speciation] {#sec-basic-filtering}

We will begin by creating filters for a VCF file consisting of variant
sites only for *red* and *yellow* ecotypes. Before we start, let's
review some statistics for the entire (unfiltered) call set:

```{bash }
#| label: bcftools-stats-variantsites
#| echo: true
#| eval: true
bcftools stats variantsites.vcf.gz | grep ^SN
```

Keep track of these numbers as we will use them to evaluate the
effects of filtering.

### When to remove complex variants?

As you can see, the raw variant data set contains indels as well as
multiallelic SNPs. Often we are mainly concerned with SNPs, in
particular bi-allelic SNPs, which begs the question: when should we
remove indels and multiallelic sites; prior to or after quality
filtering? This will depend on the research question, but a general
recommendation is to filter on quality measures first, and then remove
more complex variants. This is the approach we opt for here.

### Generate random subset of variants

Depending on the size of a study, both in terms of reference sequence
and number of samples, the VCF output can become large; a VCF file may
in fact contain millions of variants and be several hundred GB! We
want to create filters by examining distributions of VCF quality
metrics and setting reasonable cutoffs. In order to decrease run time,
we will look at a random sample of sites. We use the `vcflib` program
`vcfrandomsample` to randomly sample approximately 100,000 sites from
our VCF file^[We select a fix number that should contain enough
information to generate reliable statistics. This number should not
change significantly even when files contain vastly different numbers
of sites, which is why we need adjust the parameter *r* to the number
of sites in the file.]:

```{bash }
#| label: vcflib-random-subsample
#| echo: true
#| eval: true
# Set parameter r = 100000 / total number of variants
bcftools view variantsites.vcf.gz | vcfrandomsample -r 0.9 |\
    bgzip -c > variantsites.subset.vcf.gz
bcftools index variantsites.subset.vcf.gz
bcftools stats variantsites.subset.vcf.gz |\
    grep "number of records:"
```

The `-r` parameter sets the *rate* of sampling which is why we get
*approximately* 100,000 sites. You will need to adjust this parameter accordingly.

We will now use `vcftools` to compile statistics. By default,
`vcftools` outputs results to files with a prefix `out.` in the
current directory. You can read up on settings and options by
consulting the man pages with `man vcftools`^[`vcftools` does **not**
have a `-h` or `--help` option.]. Therefore, we define a variable
`OUT` where we will output our quality metrics, along with a variable
referencing our variant subset:

```{bash }
#| label: vcftools-set-out-prefix
#| echo: true
#| eval: true
mkdir -p vcftools
OUT=vcftools/variantsites.subset
VCF=variantsites.subset.vcf.gz
```

```{r }
#| label: set-vcf-envvars
#| echo: false
#| eval: true
#| cache: false
Sys.setenv(
    OUT = "vcftools/variantsites.subset",
    VCF = "variantsites.subset.vcf.gz",
    OUTVCF = "variantsites.filtered.vcf.gz"
)
```

### Generate statistics for filters

`vcftools` can compile many different kinds of statistics. Below we
will focus on the ones relevant to our data filters. We will generate
metrics and plot results as we go along, with the goal of generating a
set of filtering thresholds to apply to the data.

#### Total depth per site

To get a general overview of depth of coverage, we first generate the
average depth per *sample*^[The `2>/dev/null` outputs messages from
`vcftools` to a special file `/dev/null` which is a kind of electronic
dustbin.]:

```{bash }
#| label: vcftools-depth
#| echo: true
#| eval: true
vcftools --gzvcf $VCF --depth --out $OUT 2>/dev/null
cat ${OUT}.idepth
csvtk summary -t -f MEAN_DEPTH:mean ${OUT}.idepth
```

```{r }
#| label: r-csvtk-vcftools-depth-per-site-mean-sd
#| echo: false
#| eval: true
data <- read.table("vcftools/variantsites.subset.idepth", header=TRUE)
IDEPTH_MEAN <- round(mean(data$MEAN_DEPTH), 1)
idepth <- sprintf("%.1fX", IDEPTH_MEAN)
```

The average coverage over all samples is `r idepth`. This actually is
in the low range for a protocol based on explicitly calling genotypes.
At 5X coverage, there may be a high probability that only one of the
alleles has been sampled [@nielsen_GenotypeSNPCalling_2011], whereby
sequencing errors may be mistaken for true variation.

Then we calculate depth per site to see if we can identify reasonable depth cutoffs:

```{bash }
#| label: vcftools-depth-per-site
#| echo: true
#| eval: true
vcftools --gzvcf $VCF --site-depth --out $OUT 2>/dev/null
head -n 3 ${OUT}.ldepth
```

So, for each position, we have a value (column `SUM_DEPTH`) for the
total depth across all samples.

We plot the distribution of total depths by counting how many times
each depth is observed. This can be done with `csvtk summary` where we
count positions and group (`-g`) by the `SUM_DEPTH` value:^[On viewing
`csvtk` plots: either you can **redirect** (`>`) the results from
`csvtk` to a png output file, or you can **pipe** (`|`) it to the
command `display` (replace `> $OUT.ldepth.png` by `| display`, which
should fire up a window with your plot.]

```{bash }
#| label: csvtk-plot-vcftools-depth-per-site
#| echo: true
#| eval: true
csvtk summary -t -g SUM_DEPTH -f POS:count -w 0 ${OUT}.ldepth |\
 csvtk sort -t -k 1:n |\
 csvtk plot line -t - -x SUM_DEPTH -y POS:count \
    --point-size 0.01 --xlab "Depth of coverage (X)" \
    --ylab "Genome coverage (bp)" \
    --width 9.0 --height 3.5 > $OUT.ldepth.png
```

::: {#fig-csvtk-plot-vcftools-depth-per-site attr-output='.details summary="Output"'}

![](vcftools/variantsites.subset.ldepth.png)

Distribution of the total depth per site for all samples.

:::

::: {.callout-note collapse=true}

#### On csvtk as plotting software

You are of course perfectly welcome to use `R` or some other software
to make these plots. We choose to generate the plots using `csvtk` to
avoid too much context switching, and also because it emulates much of
the functionality in `R`, albeit much less powerful when it comes to
plotting.

:::

As @fig-csvtk-plot-vcftools-depth-per-site shows, most sites fall
within a peak, but also that there are sites with very high coverage,
up to ten times as high as the depth at the peak maximum. We calculate
some range statistics to get an idea of the spread. The following
`csvtk` command will calculate the minimum, first quartile, median,
mean, third quartile, and maximum of the third column (`SUM_DEPTH`):

```{bash }
#| label: csvtk-vcftools-depth-per-site-quartiles
#| echo: true
#| eval: true
csvtk summary -t -f 3:min,3:q1,3:median,3:mean,3:q3,3:max,3:stdev vcftools/variantsites.subset.ldepth
```

```{r }
#| label: r-ldepth-summary
#| echo: false
#| eval: true
df.ldepth <- read.table("vcftools/variantsites.subset.ldepth", header=TRUE)
x <- as.vector(summary(df.ldepth$SUM_DEPTH)[c(2,5)])
iqr <- sprintf("%i-%i", x[1], x[2])
```

The range from the first quartile (`q1`) to the third (`q3`) is
`r iqr`, showing most sites have a depth between 50-100X. We redraw
the plot to zoom in on the peak:

```{bash }
#| label: csvtk-plot-vcftools-depth-per-site-zoom-in
#| echo: false
#| eval: true
csvtk summary -t -g SUM_DEPTH -f POS:count -w 0 ${OUT}.ldepth |\
 csvtk sort -t -k 1:n |\
 csvtk plot line -t - -x SUM_DEPTH -y POS:count \
    --point-size 0.01 --xlab "Depth of coverage (X)" \
    --ylab "Genome coverage (bp)" \
    --width 9.0 --height 3.5  --x-min 0 --x-max 140 --y-max 2500 > $OUT.ldepth.zoomin.png
```

::: {#fig-csvtk-plot-vcftools-depth-per-site-zoom-in attr-output='.details summary="Output"'}

![](vcftools/variantsites.subset.ldepth.zoomin.png)

Zoomed in version of @fig-csvtk-plot-vcftools-depth-per-site which was
achieved by adding the options `--x-min 0 --x-max 140 --y-max 2500` to
the plotting call.

:::

We could choose a filter based on the quantile statistics above, or by
eye-balling the graph. In this example, we could have chosen the range
50-150X, which equates to 5-15X depth *per sample*; note that your
values will probably be different.

As an aside, we mention that there is a command to directly get the
per-site mean depth, `--site-mean-depth`:

```{bash }
#| label: vcftools-mean-depth-per-site
#| echo: true
#| eval: true
vcftools --gzvcf $VCF --site-mean-depth --out $OUT 2>/dev/null
head -n 3 ${OUT}.ldepth.mean
```

#### Variant quality distribution

Another quantity of interest is the variant quality. Recall, variant
quality scores are Phred-scaled scores, meaning a value of 10 has a
10% chance of being wrong, 20 a 1% chance, and so on; you typically
want to at least filter on 20 or even higher. We extract and plot the
quality values below:

```{bash }
#| label: vcftools-quality
#| echo: true
#| eval: true
#| results: hide
vcftools --gzvcf $VCF --site-quality --out $OUT
```

```{bash }
#| label: cvstk-vcftools-site-quality
#| echo: true
#| eval: true
# To improve histogram, filter out extreme quality scores. You
# may have to fiddle with the exact values
csvtk filter -t -f "QUAL>0" -f "QUAL<1000" ${OUT}.lqual  | \
 csvtk summary -t -g QUAL -f POS:count -w 0 - |\
 csvtk sort -t -k 1:n |\
 csvtk plot hist -t --bins 100 - \
          --xlab "Quality value" \
    --ylab "Count" \
    --width 9.0 --height 3.5 > $OUT.lqual.png
```

::: {#fig-csvtk-vcftools-site-quality attr-output='.details summary="Output"'}

![](vcftools/variantsites.subset.lqual.png)

Distribution of variant quality scores.

:::

Clearly most quality scores are above 20-30. For many applications, we
**recommend setting 30 as the cutoff**.

#### Minor allele frequency distribution

Since we are going to calculate nucleotide diversities, we will not
filter on the minor allele frequency (MAF) here. Nevertheless, we
generate the distribution and plot for discussion purposes. The
`--freq2` will output the frequencies only, adding the option
`--max-alleles 2` to focus only on bi-allelic sites:

```{bash }
#| label: vcftools-maf
#| echo: true
#| eval: true
vcftools --gzvcf $VCF --freq2 --out $OUT --max-alleles 2 2>/dev/null
head -n 3 ${OUT}.frq
```

The last two columns are frequencies ordered according to the
reference allele. Therefore, we need to pick the minimum value to get
the MAF. We can use `csvtk mutate` to create a new column

```{bash }
#| label: cvstk-vcftools-minor-allele-frequency
#| echo: true
#| eval: true
csvtk fix -t ${OUT}.frq 2>/dev/null |\
 csvtk mutate2 -t -n maf -e '${5} > ${6} ? "${6}" : "${5}" ' - |\
 csvtk plot hist -t --bins 20 -f maf - \
       --xlab "Minor allele frequency" \
       --ylab "Count" \
       --width 9.0 --height 3.5 > $OUT.frq.png
```

::: {#fig-cvstk-vcftools-minor-allele-frequency attr-output='.details summary="Output"'}

![](vcftools/variantsites.subset.frq.png)

Distribution of minor allele frequencies.

:::

Since our variant file consists of 10 individuals, that is, 20
chromosomes, there are only so many frequencies that we can observe,
which is why the histogram looks a bit disconnected^[You can try
different values of the `--bins` option]. In fact, given 20
chromosomes, MAF=0.05 corresponds to one alternative allele among all
individuals (*singleton*), MAF=0.1 to two, and so on. The maximum
value is 0.5, which is to be expected, as it is a *minor* allele
frequency. We note that there are more sites with a low minor allele
frequency, which in practice means there are many singleton variants.

This is where filtering on MAF can get tricky. Singletons may
correspond to sequencing error, but if too hard a filter is applied,
the resulting site frequency spectrum (SFS) will be skewed. For
statistics that are based on the SFS, this may lead biased estimates.
Since we will be applying such a statistic, we do not filter on the
MAF here. Note, however, that for other applications, such as
population structure, it may be warranted to more stringently (say,
MAF>0.1) filter out low-frequency variants.

#### Missing data for individuals and sites

The proportion missing data per individual can indicate whether the
input DNA was of poor quality, and that the individual should be
excluded from analysis. Note that in this case, missing data refers to
a missing genotype call and not sequencing depth!

We can calculate the proportion of missing data

```{bash }
#| label: vcftools-missing-data-individual
#| echo: true
#| eval: true
vcftools --gzvcf $VCF --missing-indv --out $OUT 2>/dev/null
head -n 3 ${OUT}.imiss
```

and look at the results, focusing on the `F_MISS` column (proportion missing sites):

```{bash }
#| label: cvstk-vcftools-proportion-missing-ind
#| echo: true
#| eval: true
csvtk plot hist -t --x-min 0 -f F_MISS ${OUT}.imiss > ${OUT}.imiss.png
```

::: {#fig-cvstk-vcftools-proportion-missing-ind output='.details summary="Output"'}

![](vcftools/variantsites.subset.imiss.png)

Distribution of missingness per sample.

:::

Here, the proportion lies in the range 0.06-0.10 for all samples,
which indicates good coverage of all samples and we refrain from
taking any action.

Similarly, we can look at missingness per site. This is related to the
filter based on minimum number of individuals suggested by
@lou_BeginnerGuideLowcoverage_2021. We calculate

```{bash }
#| label: vcftools-missing-data-site
#| echo: true
#| eval: true
vcftools --gzvcf $VCF --missing-site --out $OUT 2>/dev/null
head -n 3 ${OUT}.lmiss
```

and plot to get an idea of if there are sites that lack data.

```{bash }
#| label: cvstk-vcftools-proportion-missing-site
#| echo: true
#| eval: true
csvtk plot hist --bins 20 -t -f F_MISS ${OUT}.lmiss > ${OUT}.lmiss.png
```

::: {#fig-cvstk-vcftools-proportion-missing-site output='.details summary="Output"'}

![](vcftools/variantsites.subset.lmiss.png)

Distribution of missingness among sites.

:::

As @fig-cvstk-vcftools-proportion-missing-site shows, many sites have
no or little missing data, but given the low coverage, there is a
non-negligible number of sites with higher missingness. We calculate
range statistics to get a feeling for a good cutoff:

```{bash }
#| label: cvstk-missing-sites-cutoff
#| echo: true
#| eval: true
csvtk summary -t -f 6:min,6:q1,6:median,6:mean,6:q3,6:max vcftools/variantsites.subset.lmiss
```

The mean missingness is 8%, so we can safely use 25% missingness as
threshold. Typical values of tolerated missingness lie in the range
5-25%. Note that `vcftools` interprets this value as 1 - missingness,
so it has to be inverted to 75% when filtering!

#### Heterozygosity

`vcftools` can calculate the heterozygosity per individual. More
specifically, it estimates the [inbreeding coefficient
F](https://en.wikipedia.org/wiki/F-statistics) for each individual.

```{bash }
#| label: vcftools-heterozygosity
#| echo: true
#| eval: true
vcftools --gzvcf $VCF --het --out $OUT 2>/dev/null
cat ${OUT}.het
```

Here, F is a measure of how much the observed homozygotes `O(HOM)`
differ from the expected (`E(HOM)`; expected *by chance* under
Hardy-Weinberg equilibrium), and may be negative. `vcftools`
calculates F from the expression
$F=(O-E)/(N-E)$^[@purcell_PLINKToolSet_2007, p. 565 gives a coherent
derivation of this estimator], which you can verify by substituting
the variables in the output.

If F is positive ($O(HOM) > E(HOM)$), i.e., there are more observed
homozygotes than expected, then there is a deficit of heterozygotes,
which could be a sign of inbreeding or signs of allelic dropout in
case of low sequencing coverage.

If F is negative, there are fewer observed homozygotes than expected,
or conversely, an excess of heterozygotes. This could be indicative of
poor sequence quality (bad mappings) or contamination
[@purcell_PLINKToolSet_2007].

The underlying assumption is HWE, which holds for F=0.

In this case, we know that the samples are from two different
populations, red and yellow. In such cases, we actually expect a
deficit of heterozygotes (and consequently, positive F) simply due to
something called the [Wahlund
effect](https://en.wikipedia.org/wiki/Wahlund_effect).

::: {.callout-warning}

The inbreeding coefficient is a population-level statistic and is not
reliable for small sample sizes ($n<10$, say). Therefore, our sample
size is in the lower range and the results should be taken with a
grain of salt.

:::

::: {.callout-exercise}

Use `bcftools view -s SAMPLENAMES | vcftools --vcf - --het --stdout` to
calculate the heterozygosity for red and yellow samples. Substitute
`SAMPLENAMES` for a comma-separated list of samples.

::: {.callout-answer}

```{bash }
#| label: vcftools-heterozygosity-red
#| echo: true
#| eval: true
bcftools view -s PUN-R-ELF,PUN-R-JMC,PUN-R-LH,PUN-R-MT,PUN-R-UCSD $VCF |\
 vcftools --vcf - --het --stdout 2>/dev/null
```

:::

:::

### Filtering the VCF

Now that we have decided on filters we can apply them to the VCF. We
first set the filters as variables:

```{bash }
#| label: vcf-filter-variables
#| echo: true
#| eval: true
MISS=0.75
QUAL=30
MIN_DEPTH=5
MAX_DEPTH=15
```

```{r }
#| label: vcf-filter-variables-save
#| echo: false
#| eval: true
Sys.setenv(
    MISS = 0.75,
    QUAL = 30,
    MIN_DEPTH = 5,
    MAX_DEPTH = 15
)
```

and run `vcftools` as follows:

```{bash }
#| label: vcftools-apply-filters
#| echo: true
#| eval: true
OUTVCF=${VCF%.subset.vcf.gz}.filtered.vcf.gz
vcftools --gzvcf $VCF \
   --remove-indels --max-missing $MISS \
   --min-meanDP $MIN_DEPTH --max-meanDP $MAX_DEPTH \
   --minDP $MIN_DEPTH --maxDP $MAX_DEPTH --recode \
   --stdout 2>/dev/null |
 gzip -c > $OUTVCF
```

Compare the results with the original input:

```{bash }
#| label: bcftools-compare-filter-to-original
#| echo: true
#| eval: true
bcftools stats $OUTVCF | grep "^SN"
```

Quite a substantial portion variants have in fact been removed, which
here can most likely be attributed to the low average sequencing
coverage.

## Conclusion

Congratulations! You have now gone through a set of tedious and
complex steps to generate output files that determine what regions in
a reference DNA sequence are amenable to analysis. In the next
exercise we will use these files as inputs to different programs that
calculate diversity statistics from population genomic data.

## References

[^speciation]: This exercise is inspired by and based on <https://speciationgenomics.github.io/filtering_vcfs/>]
